{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (1) Метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним формулировку метода главных компонент. $X$ - матрица данных размерности $(n,p)$ с $n$ наблюдениями и $p$ признаками, $W$ - отображение в базис главных компонент, $\\Lambda$ - диагональная матрица из спектрального разложения (на диагонали стоят собственные значения). Нахождение отображения $W$ сводится к решению системы уравнений:\n",
    "$$Cov(X,X)W=W\\Lambda $$\n",
    "\n",
    "Найдите решение PCA и выведите матрицу, у которой каждой строчке соответствует компонента $W$, домноженная на корень собственного значения. Порядок строчек по убыванию собственных значений.\n",
    "\n",
    "Найдите честное решение, в том числе посчитайте матрицу ковариаций с помощью матричного умножения. Для нахождения спектрального разложения воспользуйтесь методом [np.linalg.eig](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html). Совет: обратите внимание на формат вывода метода eig и не забудьте отцентрировать данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 1\n",
    "#### Input\n",
    "```python\n",
    "X = np.array([[5.1, 3.5, 1.4, 0.2],\n",
    "              [4.9, 3. , 1.4, 0.2],\n",
    "              [4.7, 3.2, 1.3, 0.2],\n",
    "              [4.6, 3.1, 1.5, 0.2],\n",
    "              [5. , 3.6, 1.4, 0.2],\n",
    "              [5.4, 3.9, 1.7, 0.4],\n",
    "              [4.6, 3.4, 1.4, 0.3],\n",
    "              [5. , 3.4, 1.5, 0.2],\n",
    "              [4.4, 2.9, 1.4, 0.2],\n",
    "              [4.9, 3.1, 1.5, 0.1],\n",
    "              [5.4, 3.7, 1.5, 0.2],\n",
    "              [4.8, 3.4, 1.6, 0.2],\n",
    "              [4.8, 3.,  1.4, 0.1],\n",
    "              [4.3, 3.,  1.1, 0.1],\n",
    "              [5.8, 4.,  1.2, 0.2],\n",
    "              [5.7, 4.4, 1.5, 0.4],\n",
    "              [5.4, 3.9, 1.3, 0.4],\n",
    "              [5.1, 3.5, 1.4, 0.3],\n",
    "              [5.7, 3.8, 1.7, 0.3],\n",
    "              [5.1, 3.8, 1.5, 0.3]])\n",
    "```\n",
    "#### Output\n",
    "```python\n",
    "FindPCA(X) == np.array(\n",
    "    [[ 1.80580188,  1.71516927,  0.20432835, 0.27950407],\n",
    "     [ 0.41552965, -0.44413871,  0.25044627, -0.14226775],\n",
    "     [ 0.1761697 , -0.09586739, -0.54736968, -0.14974962],\n",
    "     [ 0.02964646, -0.06119331, -0.03759026,  0.2114531 ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPCA(X):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindPCA(X):\n",
    "    X_c = X - X.mean(axis=0)\n",
    "    cov = X_c.T @ X_c\n",
    "    v, w = np.linalg.eig(cov)\n",
    "    order = np.argsort(-v)\n",
    "    return (np.diag(np.sqrt(v)) @ w.T)[order]\n",
    "\n",
    "def rotate(W):\n",
    "    return W * np.sign(W)[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.54064876  0.27963865  0.10603379 -0.00667862]\n",
      " [-4.31275507 -0.29889167 -0.05770109  0.01378534]\n",
      " [-0.5137791   0.16854262 -0.32945327  0.00846816]\n",
      " [-0.70280678 -0.09574181 -0.09013196 -0.04763517]]\n",
      "[[-4.54064876  0.27963865  0.10603379 -0.00667862]\n",
      " [-4.31275507 -0.29889167 -0.05770109  0.01378534]\n",
      " [-0.5137791   0.16854262 -0.32945327  0.00846816]\n",
      " [-0.70280678 -0.09574181 -0.09013196 -0.04763517]]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "X = np.array([[5.1, 3.5, 1.4, 0.2],\n",
    "              [4.9, 3. , 1.4, 0.2],\n",
    "              [4.7, 3.2, 1.3, 0.2],\n",
    "              [4.6, 3.1, 1.5, 0.2],\n",
    "              [5. , 3.6, 1.4, 0.2],\n",
    "              [5.4, 3.9, 1.7, 0.4],\n",
    "              [4.6, 3.4, 1.4, 0.3],\n",
    "              [5. , 3.4, 1.5, 0.2],\n",
    "              [4.4, 2.9, 1.4, 0.2],\n",
    "              [4.9, 3.1, 1.5, 0.1],\n",
    "              [5.4, 3.7, 1.5, 0.2],\n",
    "              [4.8, 3.4, 1.6, 0.2],\n",
    "              [4.8, 3.,  1.4, 0.1],\n",
    "              [4.3, 3.,  1.1, 0.1],\n",
    "              [5.8, 4.,  1.2, 0.2],\n",
    "              [5.7, 4.4, 1.5, 0.4],\n",
    "              [5.4, 3.9, 1.3, 0.4],\n",
    "              [5.1, 3.5, 1.4, 0.3],\n",
    "              [5.7, 3.8, 1.7, 0.3],\n",
    "              [5.1, 3.8, 1.5, 0.3]])\n",
    "u, l, w = np.linalg.svd(X - X.mean(axis=0))\n",
    "\n",
    "X_c = X - X.mean(axis=0)\n",
    "cov = X_c.T @ X_c\n",
    "print(cov @ w.T)\n",
    "print(w.T @ np.diag(l)**2)\n",
    "\n",
    "true = np.array(\n",
    "    [[ 1.80580188,  1.71516927,  0.20432835, 0.27950407],\n",
    "     [ 0.41552965, -0.44413871,  0.25044627, -0.14226775],\n",
    "     [ 0.1761697 , -0.09586739, -0.54736968, -0.14974962],\n",
    "     [ 0.02964646, -0.06119331, -0.03759026,  0.2114531 ]])\n",
    "\n",
    "assert_array_almost_equal(rotate(true), rotate(FindPCA(X)))\n",
    "######################################################\n",
    "X = np.diag([1,2,3,4])\n",
    "\n",
    "true = np.array(\n",
    "    [[ 1.66260670e-01,  4.37167349e-01,  1.37909923e+00,\n",
    "        -3.37817635e+00],\n",
    "     [ 2.88297587e-01,  1.21501293e+00, -2.15589188e+00,\n",
    "        -7.08693698e-01],\n",
    "     [ 7.99526041e-01, -1.15439521e+00, -4.47454512e-01,\n",
    "        -2.92707730e-01],\n",
    "     [ 3.01652590e-08,  1.50826295e-08,  1.00550863e-08,\n",
    "         7.54131476e-09]])\n",
    "\n",
    "assert_array_almost_equal(rotate(true), rotate(FindPCA(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (2) tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функционал, который tSNE оптимизирует во время обучения - это [расстояние Кульбака - Лейблера](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) между вероятностным распределением в исходных данных и распределением после трансформации. Эта метрика определяется так:\n",
    "$$D_{KL}(P\\parallel Q)=\\sum_{i=1}^{n}p_i \\log(\\frac{p_i}{q_i})$$\n",
    "Реализуйте метод, который считает KL-divergence. При наличии нулей во втором векторе должен возвращаться np.inf. Вообще, обратите внимание на крайние случаи и напишите пользуясь только numpy движком. Если векторы не являются распределениями вероятности, сделайте с ними необходимую трансформацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 1\n",
    "#### Input\n",
    "```python\n",
    "p, q = [0.5,0.5], [1,0]\n",
    "```\n",
    "#### Output\n",
    "```python\n",
    "KLDivergence(p,q) == np.inf\n",
    "```\n",
    "### Sample 2\n",
    "#### Input\n",
    "```python\n",
    "p, q = [0.2, 0.1, 0., 0.7], [0.4, 0.1, 0.1, 0.4]\n",
    "```\n",
    "#### Output\n",
    "```python\n",
    "KLDivergence(p,q) == 0.2531\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDivergence(p,q):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDivergence(p, q):\n",
    "    p = np.array(p) / np.sum(p)\n",
    "    q = np.array(q) / np.sum(q)\n",
    "    log = np.log(p/q)\n",
    "    log[np.where(p==0)] = 0\n",
    "    return p @ log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "p, q = [0.5,0.5], [1,0]\n",
    "\n",
    "assert_almost_equal(KLDivergence(p,q), np.inf)\n",
    "######################################################\n",
    "p, q = [0.2, 0.1, 0., 0.7], [0.4, 0.1, 0.1, 0.4]\n",
    "assert_almost_equal(KLDivergence(p,q), 0.25310161544280674)\n",
    "######################################################\n",
    "p, q = [0, 0, 0, 1], [0.3333, 0.3333, 0.3333, 0.3333]\n",
    "assert_almost_equal(KLDivergence(p,q),1.3862943611198906)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (3) LLoyd's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (4) Elkan's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLIDE (5) DBSCAN 'knee' epsilon selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl(P, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4737363769318541"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl(Q, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.44653167389282"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl([0,1,2,3],[1,0.000000000000000001,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[np.where(Q==0)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.063320551950169"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.entropy([0.5,0.5],qk=[0.00001,0.99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-inf,  inf])"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([0,1])\n",
    "b=np.array([1,0])\n",
    "np.log(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
